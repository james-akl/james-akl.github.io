<!doctype html><html lang=en><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Principled testing ~ James Akl</title><link rel=canonical href=https://jamesakl.com/posts/principled-testing/><link rel=icon href=/favicon.ico sizes=any><link rel=icon href=/favicon.svg type=image/svg+xml><meta name=description content="Testing software-intensive systems involves more decisions than the standard vocabulary suggests. The conventional taxonomy (unit, integration, system, …"><meta name=author content="James Akl"><meta name=robots content="index, follow"><meta property="og:type" content="article"><meta property="og:url" content="https://jamesakl.com/posts/principled-testing/"><meta property="og:title" content="Principled testing | James Akl"><meta property="og:description" content="Testing software-intensive systems involves more decisions than the standard vocabulary suggests. The conventional taxonomy (unit, integration, system, …"><meta property="og:site_name" content="James Akl"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2026-02-15T00:00:00Z"><meta property="article:modified_time" content="2026-02-15T00:00:00Z"><meta property="article:author" content="James Akl"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Principled testing","description":"Testing software-intensive systems involves more decisions than the standard vocabulary suggests. The conventional taxonomy (unit, integration, system, …","url":"https:\/\/jamesakl.com\/posts\/principled-testing\/","datePublished":"2026-02-15T00:00:00Z","dateModified":"2026-02-15T00:00:00Z","author":{"@type":"Person","name":"James Akl","url":"https:\/\/jamesakl.com"},"publisher":{"@type":"Person","name":"James Akl","url":"https:\/\/jamesakl.com"}}</script><link rel=alternate type=application/rss+xml title="James Akl" href=https://jamesakl.com/index.xml><link rel=sitemap type=application/xml title=Sitemap href=https://jamesakl.com/sitemap.xml><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@300;400;500;600&family=Roboto+Mono:wght@400;500;600;700&display=swap" rel=stylesheet><style>:root{--primary-font:'Source Code Pro', sans-serif;--secondary-font:'Roboto Mono', monospace}[style*="var(--secondary-font)"],.mono{font-feature-settings:"kern" 1,"liga" 0,"calt" 1;letter-spacing:0}body{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}@media(-webkit-min-device-pixel-ratio:2),(min-resolution:192dpi){body{-webkit-font-smoothing:antialiased}}</style><style>:root{--bg-primary:#0d0d0d;--bg-secondary:#1a1a1a;--bg-tertiary:#242424;--text-primary:#e6e6e6;--text-secondary:#b0b0b0;--text-muted:#707070;--accent-primary:#60a5fa;--accent-secondary:#86efac;--accent-tertiary:#fbbf24;--border-subtle:#2a2a2a;--border-focus:#3f3f3f;--shadow:rgba(0, 0, 0, 0.3)}body{font-family:var(--primary-font);font-size:.9375rem;line-height:1.75;font-weight:400;text-rendering:geometricPrecision;-webkit-font-smoothing:subpixel-antialiased;-moz-osx-font-smoothing:auto;font-feature-settings:"kern" 1,"liga" 0,"calt" 1;padding:0;margin:0;background:var(--bg-primary);color:var(--text-primary);min-height:100vh;overflow-x:hidden;overflow-y:scroll}.site-container{max-width:75ch;margin:0 auto;padding:2rem 1.5rem;min-height:100vh;display:flex;flex-direction:column;box-sizing:border-box}.content{flex:1}h1,h2,h3,h4,h5,h6{font-family:var(--secondary-font);line-height:1.2;font-weight:600;letter-spacing:-.015em}h1{font-size:2rem;margin:0 0 .75rem;color:var(--text-primary);font-weight:700}h2{font-size:1.5rem;color:var(--text-primary);margin:3rem 0 1rem;padding-top:1.5rem;border-top:1px solid var(--border-subtle);font-weight:700}h2:first-of-type{border-top:none;padding-top:0;margin-top:2rem}h3{font-size:1.2rem;margin:2.5rem 0 1rem;font-weight:600;color:var(--text-primary)}h4{font-size:1rem;color:var(--accent-secondary);font-weight:600;margin:2rem 0 .75rem}p{margin:1rem 0;color:var(--text-primary)}strong{color:var(--accent-secondary);font-weight:600}em{font-style:italic}::selection{background:var(--accent-primary);color:var(--bg-primary)}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:var(--bg-primary)}::-webkit-scrollbar-thumb{background:var(--border-focus);border-radius:4px}::-webkit-scrollbar-thumb:hover{background:var(--text-muted)}a{color:var(--accent-primary);text-decoration:none;transition:all .2s ease;border-radius:2px}a:hover{color:#8bb8e8}a:focus{outline:2px solid var(--accent-primary);outline-offset:2px}.site-header{display:flex;justify-content:space-between;align-items:center;padding:1rem 0 1.5rem;border-bottom:1px solid var(--border-subtle);margin-bottom:1.5rem}.site-title{font-family:var(--secondary-font);font-size:1rem;font-weight:400;color:var(--text-primary);text-decoration:none}.site-title:hover{color:var(--accent-primary)}.site-nav{display:flex;flex-wrap:wrap;gap:.25rem .75rem;align-items:center}.nav-link{font-family:var(--secondary-font);font-size:1rem;color:var(--text-primary);text-decoration:none;transition:color .2s ease}.nav-link:hover{opacity:1;color:var(--accent-primary)}.nav-icon{opacity:.7;transition:all .2s ease}.nav-icon:hover{opacity:1;transform:translateY(-1px)}.nav-icon svg{fill:var(--accent-secondary);transition:fill .2s ease}.sr-only{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;clip:rect(0,0,0,0);border:0}.nav-icon:hover svg{fill:#a0f5bf}.content-section{margin:1.5rem 0}hr{border:none;height:1px;background:linear-gradient(to right,transparent,var(--border-subtle),transparent);margin:2rem 0}strong{color:var(--accent-secondary);font-weight:500}.section-table{display:grid;grid-template-columns:4rem 1fr auto;gap:.25rem 1rem;margin-top:1.5rem;align-items:baseline}.section-label{font-family:var(--secondary-font);font-size:.7rem;font-weight:400;color:var(--text-muted);text-transform:uppercase;letter-spacing:.04em;text-align:right}.section-title{font-family:var(--secondary-font);font-size:1rem;color:var(--text-primary);text-decoration:none}.section-title:hover{color:var(--accent-primary)}.section-meta{font-family:var(--secondary-font);font-size:.9rem;color:var(--text-muted);text-align:right}.section-about{grid-column:span 2;font-size:1rem;color:var(--text-secondary);line-height:1.6;text-align:justify;hyphens:auto;-webkit-hyphens:auto}.section-about p{margin:0}.section-spacer{grid-column:span 3;height:.75rem}@media(max-width:500px){.section-table{display:grid;grid-template-columns:1fr auto;gap:.25rem .75rem}.section-label{grid-column:span 2;text-align:left;margin-top:1rem}.section-label:first-child{margin-top:0}.section-label:empty{display:none}.section-about{grid-column:span 2}.section-spacer{grid-column:span 2;height:.5rem}}.project-list{font-family:var(--secondary-font)}.project-item{display:flex;justify-content:space-between;align-items:center;padding:.25rem 0;line-height:1.4}.project-link{color:var(--text-primary);text-decoration:none}.project-link:hover{color:var(--accent-primary)}.project-desc{color:var(--text-muted);font-size:.9rem}.post-list{margin:0;font-family:var(--secondary-font)}.post-item{display:flex;justify-content:space-between;align-items:center;padding:.25rem 0;line-height:1.4;gap:1rem}.post-meta{display:flex;align-items:center;gap:1rem;white-space:nowrap}.post-desc{color:var(--text-muted);font-size:.9rem}.post-date{color:var(--text-muted);font-size:.9rem}.post-line{display:flex;justify-content:space-between;align-items:center;padding:.25rem 0;line-height:1.4}.post-line:hover{background-color:rgba(255,255,255,5%);margin:0 -.5rem;padding-left:.5rem;padding-right:.5rem}.post-link{color:var(--text-primary);text-decoration:none;flex:1;margin-right:1rem}.post-link:hover{color:var(--accent-primary)}.post-date{color:var(--text-muted);font-size:.9rem;white-space:nowrap}.post-excerpt{margin:.75rem 0 0;color:var(--text-secondary);line-height:1.5}.read-more{color:var(--accent-primary);font-size:.85rem;margin-top:.5rem;display:inline-block;line-height:1.5;font-weight:400}.read-more:hover{color:var(--accent-secondary)}.post-header{margin-bottom:1.5rem}.post-navigation{display:flex;justify-content:space-between;margin:2rem 0 1rem;padding:1rem 0;border-top:1px solid var(--border-subtle);font-size:.95rem}.post-nav-prev,.post-nav-next{flex:1}.post-nav-next{text-align:right}.post-nav-label{color:var(--text-muted);font-size:.85rem;display:block;margin-bottom:.25rem}.post-nav-title{color:var(--accent-primary);font-weight:500}.post-footer{margin-top:auto;margin-bottom:1rem;padding-top:1rem;border-top:1px solid var(--border-subtle);font-size:.7rem;color:var(--text-muted);line-height:1.3}.post-footer .post-meta{justify-content:center;text-align:center;margin:0;white-space:normal}pre{background:var(--bg-secondary);border-left:3px solid var(--border-focus);border-radius:0;padding:1.25rem 1.5rem;margin:1.75rem 0;overflow-x:auto;font-family:var(--secondary-font);font-size:.875rem;line-height:1.6;font-weight:400;color:var(--text-primary);box-shadow:0 1px 3px var(--shadow)}code{font-family:var(--secondary-font);font-size:.875em;font-weight:500;background:var(--bg-secondary);color:var(--text-primary);padding:.2em .5em;margin:0;border-radius:2px;border:1px solid var(--border-subtle)}pre code{background:0 0;border:none;padding:0;margin:0;font-size:inherit;font-weight:400;color:inherit;white-space:pre}.mermaid{display:flex;justify-content:center;margin:2rem 0}.mermaid svg{max-height:500px}.mermaid .node rect{ry:3!important;rx:3!important}blockquote{margin:2rem 0;padding:1rem 1.5rem;border-left:4px solid var(--accent-secondary);background:var(--bg-secondary);font-style:italic;color:var(--text-secondary);box-shadow:0 1px 2px var(--shadow)}table{width:100%;border-collapse:collapse;margin:2rem 0;font-size:.875rem;line-height:1.5;display:table;table-layout:auto;background:var(--bg-secondary);box-shadow:0 1px 3px var(--shadow)}.content table{display:block;overflow-x:auto;-webkit-overflow-scrolling:touch}.content table table{display:table}thead{border-bottom:2px solid var(--border-focus)}th{text-align:left;padding:.75rem 1rem;font-weight:600;color:var(--accent-secondary);background:var(--bg-tertiary);white-space:nowrap;font-size:.875rem}td{padding:.65rem 1rem;border-bottom:1px solid var(--border-subtle);color:var(--text-primary);vertical-align:top}td code{font-size:.85em;padding:.15em .4em}tbody tr:hover{background:var(--bg-tertiary);transition:background .2s ease}ul,ol{margin:1.25rem 0;padding-left:2rem;line-height:1.7}li{margin:.5rem 0;color:var(--text-primary)}li>ul,li>ol{margin:.6rem 0}li::marker{color:var(--accent-secondary)}sub,sup{font-size:.75em;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}@media(max-width:640px){.site-container{padding:1.5rem 1rem}.site-header{padding:1rem 0 1.25rem}h1{font-size:1.8rem}h2{font-size:1.5rem}h3{font-size:1.2rem}.post-navigation{flex-direction:column;gap:1rem}.post-nav-next{text-align:left}.post-footer .post-meta{flex-direction:column;gap:.5rem;line-height:1.3}pre{padding:.85rem;font-size:.82rem;margin:1rem -.5rem;border-radius:0}code{font-size:.86em}table{font-size:.82rem}th,td{padding:.5rem .6rem}}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css crossorigin=anonymous><div class=site-container><header class=site-header><a class=site-title title=Home href=/>James Akl</a><nav class=site-nav><a class=nav-link href=mailto:james-akl@outlook.com>Email</a>
<a class=nav-link href=https://www.linkedin.com/in/james-akl/>LinkedIn</a>
<a class=nav-link href="https://scholar.google.com/citations?user=6gPp9TMAAAAJ">Scholar</a></nav></header><main class=content><article><header class=post-header><h1>Principled testing</h1></header><div class=post-content><p>Testing software-intensive systems involves more decisions than the standard vocabulary suggests. The conventional taxonomy (unit, integration, system, acceptance) organizes tests along a single axis of scope. But each test also involves deciding <em>why</em> to run it <span style=color:#a9a9a9>(intent)</span>, <em>where</em> to run it <span style=color:#a9a9a9>(environment)</span>, and <em>how to judge correctness</em> <span style=color:#a9a9a9>(the <a href=https://en.wikipedia.org/wiki/Test_oracle>oracle</a>: the mechanism that decides whether an output is right)</span>. Separating these decisions can clarify what a test strategy covers and where the gaps are.<p>This article builds a more complete picture of what testing involves: the gap between structural and behavioral correctness; the oracle challenges that intensify in systems that learn from data or interface with the physical world; the range from <em>ad hoc</em> exploration to formal verification; and the reality that most teams work under constraints where the question is not whether to test more, but what to invest in.<h2 id=conventional-frameworks>Conventional frameworks</h2><ul><li><strong>Unit → integration → system → acceptance.</strong> The “test levels,” ordered by scope. Myers (<a href="https://books.google.com/books/about/The_Art_of_Software_Testing.html?id=GjyEFPkMCwcC">1979</a>) introduced foundational ideas about test levels in <em>The Art of Software Testing</em>; the <a href=https://istqb.org/>ISTQB syllabus</a> later codified the four-level hierarchy as the primary organizing axis.<li><strong>The V-model.</strong> Requirements map to acceptance tests, architecture to integration tests, detailed design to unit tests. Standards like <a href=https://www.iso.org/standard/81291.html>ISO/IEC/IEEE 29119</a> define generic test processes compatible with this and other lifecycle models.<li><strong>The test pyramid.</strong> Cohn (<a href=https://www.mountaingoatsoftware.com/blog/the-forgotten-layer-of-the-test-automation-pyramid>2009</a>) proposed a cost heuristic: many unit tests, fewer service-level tests, fewer still UI tests, as described by Fowler (<a href=https://martinfowler.com/bliki/TestPyramid.html>2012</a>). Variants include the <a href=https://kentcdodds.com/blog/the-testing-trophy-and-testing-classifications>testing trophy</a> <span style=color:#a9a9a9>(Dodds, emphasizing integration tests)</span> and the testing diamond <span style=color:#a9a9a9>(wider in the middle, for service-oriented architectures)</span>.<li><strong>Test-driven development.</strong> Beck (<a href="https://books.google.com/books/about/Test_driven_Development.html?id=CUlsAQAAQBAJ">2002</a>) formalized writing tests before implementation: red, green, refactor. TDD is a design discipline as much as a testing practice; it forces interface decisions before implementation. Its strength is in domains where the oracle is clear and the specification is stable enough to write a test against.</ul><p>These frameworks capture real tradeoffs. They also compress distinct decisions into a single label. “Integration test” means one thing in embedded systems <span style=color:#a9a9a9>(testing interfaces between hardware and software components)</span> and something different in web development <span style=color:#a9a9a9>(testing interactions between services)</span>. “Hardware-in-the-loop” has a precise meaning in automotive and aerospace <span style=color:#a9a9a9>(real controller hardware running real firmware, with a simulated plant model, in real time)</span> but is used loosely elsewhere for any test involving physical hardware. This is not a failure of the vocabulary; these terms serve their communities well. But they fix one dimension and leave the rest implicit, and different communities fill in those gaps differently. Decomposing a test into its constituent decisions can help bridge this ambiguity.<div style="background:#1a1a1a;padding:1.2em 1.5em;border-radius:4px;margin:1.5em 0;border-left:3px solid #60a5fa"><p><strong>A note on terminology.</strong> Adjacent disciplines use overlapping but distinct terms for related activities. <em>Verification</em> asks whether the system meets its specification; <em>validation</em> asks whether the specification meets the actual need. These are often paired as V&V <span style=color:#a9a9a9>(<a href=https://standards.ieee.org/standard/1012-2016.html>IEEE 1012</a>, DO-178C)</span>. In modeling and simulation, particularly in defense contexts, this extends to VV&A <span style=color:#a9a9a9>(Verification, Validation, and Accreditation)</span>, where <em>accreditation</em> is the formal determination that a model is acceptable for a specific use. <em>Evaluation</em> is used broadly in machine learning for model assessment <span style=color:#a9a9a9>(often shortened to “evals”)</span>. <em>Qualification</em> and <em>certification</em> appear in hardware, pharmaceutical, and aerospace contexts for formal attestation against regulatory criteria. This article uses “testing” broadly to encompass the act of executing a system and judging its outputs, which overlaps with all of these but is not synonymous with any of them.</div><h2 id=dimensions-of-a-test>Dimensions of a test</h2><p>A test involves several decisions that can, to a useful degree, be considered separately: <strong>Scope</strong> <span style=color:#a9a9a9>(what is being tested)</span>, <strong>Intent</strong> <span style=color:#a9a9a9>(why the test exists)</span>, <strong>Environment</strong> <span style=color:#a9a9a9>(where it runs)</span>, and <strong>Oracle</strong> <span style=color:#a9a9a9>(how correctness is judged)</span>. These four are not exhaustive or perfectly independent — they interact, and there may be others worth identifying <span style=color:#a9a9a9>(test input generation strategy, for instance, interacts with all four but is not reducible to any of them)</span>. But conflating any two can cause confusion.<h3 id=scope>Scope</h3><p>What is the system-under-test?<ul><li><strong>Component</strong> <span style=color:#a9a9a9>(a single function, class, or module)</span><li><strong>Subsystem</strong> <span style=color:#a9a9a9>(a pipeline, service, or bounded context)</span><li><strong>System</strong> <span style=color:#a9a9a9>(the full deployed artifact)</span><li><strong>System-of-systems</strong> <span style=color:#a9a9a9>(multiple interacting systems, including dependencies outside the team&rsquo;s deployment authority)</span></ul><p>Scope is about <em>boundaries</em>, not code size. What counts as a unit depends on where the architecture draws its interfaces. Meszaros (<a href=http://xunitpatterns.com/>2007</a>) formalized the taxonomy of <a href=https://en.wikipedia.org/wiki/Test_double>test doubles</a> <span style=color:#a9a9a9>(stubs, mocks, fakes, dummies, spies)</span> in <em>xUnit Test Patterns</em> precisely because scope decisions require explicit management of what lies outside the boundary.<h3 id=intent>Intent</h3><p>Why is this test being run?<ul><li><strong>Regression</strong> <span style=color:#a9a9a9>(did existing behavior break?)</span><li><strong>Progression</strong> <span style=color:#a9a9a9>(does the new or changed behavior work as specified?)</span><li><strong>Exploration</strong> <span style=color:#a9a9a9>(what does the system actually do in this scenario?)</span><li><strong>Acceptance</strong> <span style=color:#a9a9a9>(does the system meet requirements?)</span></ul><p>Regression and progression are complementary. A code change affects two surfaces: existing behaviors that might break, and new behaviors that must work. A progression test, if retained, becomes a regression guard once the behavior it verified is established. The distinction matters for test design. To illustrate: adding concurrency to a module calls for progression tests for the new thread-safety requirements and regression tests for existing sequential behavior. These tests have different oracle and environment requirements, since concurrency bugs are timing-dependent and may not manifest in-process. Rothermel and Harrold (<a href=https://doi.org/10.1145/248233.248262>1997</a>) formalized safe regression test selection, showing that the problem has structure independent of the other dimensions.<div style="background:#1a1a1a;padding:1.2em 1.5em;border-radius:4px;margin:1.5em 0;border-left:3px solid #60a5fa"><p><strong>On “progression testing.”</strong> “Regression testing” is standard across ISTQB, IEEE, and ISO vocabularies. “Progression testing” is less universal — it originates from the <a href=https://www.tmap.net/wiki/regression-testing-and-progression-testing/>TMap methodology</a> (Sogeti), where it is formally defined as testing new or adapted functionality. ISTQB uses “confirmation testing” for a related but narrower concept <span style=color:#a9a9a9>(specifically re-testing a defect fix to verify the fix works, rather than testing new features generally)</span>. The term “progression” is used here to emphasize the symmetry between checking that old behavior survived and checking that new behavior works.</div><p>Exploration includes interactive, <em>ad hoc</em> testing — someone running the system in varied conditions to see what happens. It does not produce a persistent automated artifact, but it consumes budget and produces information. In early development, when interfaces are still changing, it is often the most appropriate form of testing.<p>In regulated domains <span style=color:#a9a9a9>(<em>e.g.</em>, DO-178C for avionics, ISO 26262 for automotive)</span>, acceptance testing becomes certification testing: the regulatory framework prescribes specific evidence and coverage criteria, constraining the other dimensions <span style=color:#a9a9a9>(<em>e.g.</em>, <a href=https://en.wikipedia.org/wiki/DO-178C>DO-178C</a> Level A requires <a href=https://en.wikipedia.org/wiki/Modified_condition/decision_coverage>MC/DC</a> structural coverage; <a href=https://en.wikipedia.org/wiki/IEC_61508>IEC 61508</a> SIL 4 highly recommends it)</span>.<h3 id=environment>Environment</h3><p>Where does the test execute?<ul><li><strong>In-process</strong> <span style=color:#a9a9a9>(same process as the test runner, with mocked or stubbed dependencies)</span><li><strong>Simulated</strong> <span style=color:#a9a9a9>(against a model (<em>e.g.</em>, physics, networks, patients) with varying fidelity; sometimes called <em>in-silico</em> testing)</span><li><strong>Hardware-in-the-loop</strong> <span style=color:#a9a9a9>(real hardware running real firmware, with simulated stimuli)</span><li><strong>Field</strong> <span style=color:#a9a9a9>(real hardware, real world)</span></ul><p>Environment determines <em>what phenomena the test can observe</em>. An in-process test cannot observe real-time timing faults. A simulation cannot reproduce all sensor characteristics. Each environment has a <em>fidelity envelope</em>: the set of phenomena that can, in principle, manifest during execution.<pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
    A[&quot;In-process&quot;]
    B[&quot;Simulated&quot;]
    C[&quot;HIL&quot;]
    D[&quot;Field&quot;]

    A --&gt;|&quot;+ model&quot;| B
    B --&gt;|&quot;+ real&lt;br&gt;hardware&quot;| C
    C --&gt;|&quot;+ real&lt;br&gt;world&quot;| D

    style A fill:#1a1a1a,stroke:#60a5fa,stroke-width:2px,color:#60a5fa
    style B fill:#1a1a1a,stroke:#86efac,stroke-width:2px,color:#86efac
    style C fill:#1a1a1a,stroke:#fbbf24,stroke-width:2px,color:#fbbf24
    style D fill:#1a1a1a,stroke:#f87171,stroke-width:2px,color:#f87171

    classDef default font-family:Source Code Pro
</code></pre><p>“Simulated” spans a range. In automotive and aerospace, this is formalized: model-in-the-loop <span style=color:#a9a9a9>(MIL — testing a model of the software before code generation)</span>, software-in-the-loop <span style=color:#a9a9a9>(SIL — compiled production code against a simulated environment)</span>, and processor-in-the-loop <span style=color:#a9a9a9>(PIL — real code on the target processor, simulated I/O)</span>. MIL cannot reveal bugs introduced by code generation; SIL cannot reveal bugs caused by the target processor&rsquo;s timing. In domains where these distinctions are formalized <span style=color:#a9a9a9>(<em>e.g.</em>, ISO 26262)</span>, they warrant separate treatment.<p>Much of practical test engineering involves shifting tests along the environment dimension. Recording HTTP interactions for replay, mocking database connections, or using containerized dependencies <span style=color:#a9a9a9>(<em>e.g.</em>, <a href=https://testcontainers.com/>Testcontainers</a>)</span> all convert what would be an integration test against real services into a faster, more deterministic in-process or simulated test. This gains repeatability and speed but can lose fidelity: the mock may drift from the real dependency over time, and bugs that arise from the interaction with the real service (<em>e.g.</em>, latency, partial failures, version mismatches) become invisible. <a href=https://martinfowler.com/bliki/ContractTest.html>Contract testing</a> is one response: verify that the mock and the real service agree on the interface, so that the fidelity loss is bounded. The tradeoff is general: moving a test to a cheaper environment generally narrows the fidelity envelope, and the question is whether the phenomena lost are ones that matter.<p><a href=https://en.wikipedia.org/wiki/Chaos_engineering>Chaos engineering</a> pushes in the opposite direction — deliberately injecting faults <span style=color:#a9a9a9>(<em>e.g.</em>, network partitions, service failures, resource exhaustion)</span> into a production or production-like environment to test resilience. In the dimensional framework, this is exploration or progression intent at system scope, in a degraded field environment, with property oracles <span style=color:#a9a9a9>(“does the system degrade gracefully?”)</span>. It explicitly targets the phenomena that convenient testing cannot observe.<h3 id=oracle>Oracle</h3><p>How is correctness judged?<ul><li><strong>Exact</strong> <span style=color:#a9a9a9>(a specific expected output, known in advance)</span><li><strong>Differential</strong> <span style=color:#a9a9a9>(compare against a reference implementation or a previous version)</span><li><strong>Property</strong> <span style=color:#a9a9a9>(output satisfies a constraint, without specifying the exact value)</span><li><strong>Statistical</strong> <span style=color:#a9a9a9>(output distribution satisfies criteria over many runs)</span><li><strong>Human</strong> <span style=color:#a9a9a9>(a person evaluates the output)</span></ul><p>Exact and property oracles differ in specificity: an exact oracle says “this is <em>the</em> answer” <span style=color:#a9a9a9>(<em>e.g.</em>, <code>output == 42</code>)</span>; a property oracle says “any answer satisfying this constraint is acceptable” <span style=color:#a9a9a9>(<em>e.g.</em>, <code>output > 0</code> or <code>is_sorted(output)</code>)</span>. The boundary is not always sharp; a floating-point comparison with a tolerance <span style=color:#a9a9a9>(<code>abs(output - expected) &lt; epsilon</code>)</span> looks like an exact oracle but is technically a property oracle, since it accepts a range of values. Non-deterministic systems <span style=color:#a9a9a9>(<em>e.g.</em>, concurrent programs, stochastic algorithms)</span> push further: if running the same test twice can produce different results, exact oracles on full system output become unreliable, and property or statistical oracles are often more appropriate. Despite the blurred boundary, the distinction matters: exact and property oracles have different failure modes and different sensitivity to system changes.<p>Differential oracles deserve attention. Running the same inputs through the current and previous version of a system and comparing outputs is a widely used technique <span style=color:#a9a9a9>(sometimes called golden-file testing, snapshot testing, or <a href=https://en.wikipedia.org/wiki/Characterization_test>characterization testing</a> in the legacy code context)</span>. Its interpretation depends on intent: for a pure refactoring <span style=color:#a9a9a9>(no intended behavioral change)</span>, any output difference is a regression. For a feature change, some differences are expected <span style=color:#a9a9a9>(new behavior working as intended)</span> and others are not <span style=color:#a9a9a9>(existing behavior inadvertently broken)</span>. The differential oracle reports all differences without distinguishing between them; a human or further test must decide which are acceptable. This makes differential oracles powerful for regression detection but insufficient on their own for progression testing.<p>The oracle is often the most underspecified dimension. A test can verify that a function returns the correct type and structure without necessarily checking whether the <em>content</em> is correct. The gap between <em>structural</em> correctness <span style=color:#a9a9a9>(right types, right interfaces)</span> and <em>behavioral</em> correctness <span style=color:#a9a9a9>(right outputs under the right conditions)</span> is where many real failures live. A perception module can pass all its unit tests — correct interfaces, correct data formats, correct latency — and misclassify objects in deployment, because the tests verified structure rather than behavior. Barr <em>et al.</em> (<a href=https://doi.org/10.1109/TSE.2014.2372785>2015</a>) surveyed the oracle problem and found it pervasive across software testing.<p><a href=https://en.wikipedia.org/wiki/Metamorphic_testing>Metamorphic testing</a> <span style=color:#a9a9a9>(Chen <em>et al.</em>, <a href=https://www.cse.ust.hk/faculty/scc/publ/CS98-01-metamorphictesting.pdf>1998</a>)</span> sidesteps the need for a complete oracle: instead of specifying the expected output for input $x$, specify a <em>relation</em> between outputs for related inputs <span style=color:#a9a9a9>(<em>e.g.</em>, rotating an image should not change which objects are detected)</span>. This requires domain knowledge to identify useful metamorphic relations, but it is one of the few oracle strategies that scales to systems without deterministic expected outputs. <a href=https://en.wikipedia.org/wiki/Fuzzing>Fuzzing</a> takes a complementary approach: generate large volumes of inputs <span style=color:#a9a9a9>(often random or mutation-guided)</span> and check for violations of broad properties like “does not crash” or “does not trigger undefined behavior.” Fuzzing is powerful for finding faults without specifying expected outputs, though its property oracles are typically coarse.<h2 id=oracles-for-learned-systems>Oracles for learned systems</h2><p>Machine learning systems, from classical models to large-scale foundation models, present the oracle problem in especially sharp form. The challenge is not limited to any one model architecture; it is inherent, arising from the nature of learned behavior itself.<p><strong>The held-out test set</strong> is a statistical oracle at system scope: it estimates generalization beyond the training data. But it only estimates generalization <em>to the test distribution</em>. A model that performs well on a curated benchmark can fail on data from a different sensor, geography, or season. This gap (between “passes the test set” and “works in deployment”) is structurally similar to “passes in simulation” versus “works on hardware”: an environment mismatch between where the test runs and where the system operates.<p><strong>Benchmark saturation</strong> compounds this. Widely used benchmarks can saturate (state-of-the-art models can exceed 90% accuracy), which reduces their discriminative power and drives a cycle of replacing benchmarks with harder ones. The benchmarks remain useful for baseline comparison, but high scores on them say progressively less about deployment readiness. A related concern is data contamination: models trained on internet-scale data may have encountered benchmark items during training, inflating scores without a corresponding improvement in capability.<p><strong>Generative models</strong> intensify the oracle problem further. When the system&rsquo;s output is natural language, code, or other open-ended artifacts, there is often no single correct answer. The same prompt can have many valid responses differing in tone, structure, or emphasis. For some inputs exact oracles still apply <span style=color:#a9a9a9>(<em>e.g.</em>, factual questions with unambiguous answers)</span>, but for most generative tasks the field has converged on several alternatives:<ul><li><strong>Rubric-based evaluation.</strong> Define natural-language criteria <span style=color:#a9a9a9>(<em>e.g.</em>, factual accuracy, coherence, relevance)</span> and score outputs against them, often using another model as an automated judge. This is effective at scale but inherits the judge model&rsquo;s own biases <span style=color:#a9a9a9>(<em>e.g.</em>, positional preference, sensitivity to phrasing)</span>, and the rubric itself encodes assumptions about what “good” means.<li><strong>Metamorphic relations for language.</strong> Paraphrasing a question should not change the factual content of the answer; translating and back-translating should preserve meaning. These are property oracles adapted to generative outputs, an application of Chen <em>et al.</em>&rsquo;s (<a href=https://www.cse.ust.hk/faculty/scc/publ/CS98-01-metamorphictesting.pdf>1998</a>) framework to a domain far removed from its origins.<li><strong>Red-teaming and adversarial probing.</strong> Systematically searching for inputs that elicit harmful, incorrect, or policy-violating outputs, an exploration-intent technique with a human or rubric-based oracle.</ul><div style="background:#1a1a1a;padding:1.2em 1.5em;border-radius:4px;margin:1.5em 0;border-left:3px solid #86efac"><p><strong>The common thread.</strong> Whether the system under test is a classical ML pipeline, a reinforcement learning policy, or a large language model, the fundamental challenge is the same: the oracle must judge <em>behavior</em>, not just <em>structure</em>, and the behaviors that matter most in deployment are often the hardest to specify in advance. The dimensional framework applies uniformly: the question is always what scope, intent, environment, and oracle are appropriate, even as the specific oracle strategies differ.</div><h2 id=the-test-space>The test space</h2><p>The dimensions described in the preceding sections form a space. A concrete test is a point in it; a test strategy is a distribution of effort across it:<p>$$\mathcal{T} = \text{Scope} \times \text{Intent} \times \text{Environment} \times \text{Oracle}$$<p>The space is sparse: many cells are impractical, and the dimensions are not fully independent <span style=color:#a9a9a9>(scope constrains environment; regulatory acceptance constrains oracle)</span>. Conventional terms name regions of this space by fixing some dimensions and leaving others as wildcards:<table><thead><tr><th>Term<th>Scope<th>Intent<th>Env.<th>Oracle<tbody><tr><td>Unit test<td>Component<td>—<td>In-process<td>Exact / Property<tr><td>Integration test<td>Subsystem<td>—<td>In-process / Sim.<td>—<tr><td>System test<td>System<td>—<td>Sim. / HIL / Field<td>—<tr><td>Regression test<td>—<td>Regression<td>—<td>—<tr><td>Acceptance test<td>—<td>Acceptance<td>—<td>—<tr><td>Smoke test<td>System<td>Regression<td>—<td>Exact</table><div style="background:#1a1a1a;padding:1.2em 1.5em;border-radius:4px;margin:1.5em 0;border-left:3px solid #fbbf24"><p><strong>Reading this table.</strong> Each <code>—</code> is an unspecified dimension — a decision the term leaves implicit. These mappings describe <em>typical</em> usage, not definitions; they are approximate by design. A “unit test” usually uses an exact oracle <span style=color:#a9a9a9>(<code>assertEqual</code>)</span>, but property-based testing frameworks like <a href=https://en.wikipedia.org/wiki/QuickCheck>QuickCheck</a> and <a href=https://hypothesis.readthedocs.io/>Hypothesis</a> operate at component scope with property oracles. A “regression test” specifies only intent; scope, environment, and oracle are all open. The value of the table is not in fixing rigid definitions but in making visible how much conventional terms leave unspecified.</div><h2 id=recurring-challenges>Recurring challenges</h2><p>The dimensional view makes certain patterns visible.<p><strong>The structural-behavioral gap.</strong> The gap between structural and behavioral correctness, introduced in the <a href=#oracle>oracle</a> discussion above, is one of the most common blind spots in test strategies. A test suite can achieve high coverage while checking only structural properties <span style=color:#a9a9a9>(<em>e.g.</em>, correct types, interfaces, control flow)</span> without checking whether the system&rsquo;s outputs are <em>substantively correct</em>. This is an oracle problem: the tests use exact oracles on structural properties when the critical question requires property, statistical, or human oracles on behavioral properties.<p>This pattern is not unique to any particular workflow. Wherever tests are written without intimate knowledge of the system&rsquo;s intended behavior — its domain invariants, its failure modes, its edge cases — the tests tend toward structural checks, because structural properties are observable from the code alone. Behavioral oracles require understanding what the system <em>should</em> do, not just what it <em>does do</em>. This applies equally to a developer unfamiliar with the domain, a team working under time pressure, or an AI coding agent generating tests without system-level context. The issue is not who writes the test but whether the test writer (human or otherwise) has access to the domain knowledge needed to specify meaningful behavioral oracles. A passing CI pipeline built on structural tests provides less assurance than it appears to.<p><strong>Testing a moving target.</strong> In R&D, features often have not stabilized. Tests written against an immature interface break not because the system regressed but because the specification moved. The effort shifts from catching bugs to maintaining tests. Regression tests assume a stable specification; when the specification is in flux, exploration and progression are more appropriate. This is a specific case of <a href=/posts/premature-intervention/>premature intervention</a>: committing resources to a structure that has not yet settled.<p><strong>Concentrating effort where testing is convenient.</strong> Test effort naturally clusters where tooling supports it: in-process, component-scoped, exact oracles. But production failures often involve interactions between components, behavior under degraded conditions, or inputs that were never anticipated. The gap is not in coverage of the code but in coverage of the <em>phenomena</em>.<h2 id=the-limits-of-sampling>The limits of sampling</h2><p>Outside of exhaustive enumeration, testing is sampling. Every test evaluates the system on a finite set of inputs and declares the result representative. For sufficiently small finite input spaces <span style=color:#a9a9a9>(<em>e.g.</em>, a Boolean function of a few variables, or all entries in a lookup table)</span>, exhaustive enumeration is possible: the sample covers the entire space, and the test result is definitive for that space. <a href=https://en.wikipedia.org/wiki/Model_checking>Model checking</a> achieves this for finite-state systems. But for most real systems, the input space is far too large to enumerate, and any test suite is necessarily a sample. How representative that sample is depends on the relationship between the test distribution and the deployment distribution — and on the nature of the risk.<p><strong>Quantifiable risk.</strong> When failure modes are known and their probabilities estimable, statistical testing is effective. Run $n$ tests, observe $k$ failures, bound the failure rate. This is where statistical oracles and confidence intervals are meaningful.<p><strong>Statistical uncertainty.</strong> When data exists but the distribution is not fully known, bounds are weaker. A test suite may cover the observed distribution well while missing regions that have not been encountered. This is where most ML systems on real data live: a model tested on a curated dataset may perform well there and poorly on data from a different distribution.<p><strong><a href=https://en.wikipedia.org/wiki/Knightian_uncertainty>Knightian uncertainty</a>.</strong> When the failure modes themselves are unknown, no amount of testing provides guarantees. A system deployed in an uncontrolled environment faces inputs outside any test distribution. <a href=https://www.iso.org/standard/77490.html>ISO 21448 (SOTIF)</a> frames the goal as shrinking the “unknown unsafe” region, acknowledging that it cannot be eliminated.<p>These regimes require different responses. In the quantifiable regime, the question is sample size. In the statistical regime, the question is distribution coverage. In the Knightian regime, the question shifts from “does the system work correctly?” to “does the system fail safely?”, and the response moves from testing toward runtime monitoring, graceful degradation, and safety architecture. Butler and Finelli (<a href=https://doi.org/10.1109/32.210303>1993</a>) showed that for ultra-high reliability targets <span style=color:#a9a9a9>($10^{-9}$ failure rate)</span>, the required test volume is infeasible, a concrete illustration that statistical evidence from testing has fundamental limits.<h2 id=the-spectrum-of-rigor>The spectrum of rigor</h2><p>Given these limits, testing practices range widely, and most of this range is legitimate.<p>At one end: someone runs the system, tries some inputs, watches the output. Informal, unrepeatable, but in early development it may be all that makes sense. When interfaces are unstable and requirements unclear, investing in automated test infrastructure can be <a href=/posts/premature-intervention/>premature</a>.<p>In the middle: automated test suites, continuous integration, regression pipelines. Most software teams operate here. The dimensional decomposition is most useful at this level, as a tool for auditing where effort is allocated and where gaps exist. <a href=https://en.wikipedia.org/wiki/Mutation_testing>Mutation testing</a> <span style=color:#a9a9a9>(introducing small faults into the source code to check whether the test suite detects them)</span> provides a complementary lens: rather than asking “what did we test?”, it asks “what faults would our tests miss?”, a direct measure of test suite adequacy that is independent of code coverage.<p>At the other end: <a href=https://en.wikipedia.org/wiki/Formal_verification>formal verification</a>. Formal methods prove properties about the system&rsquo;s behavior for <em>all</em> possible inputs using mathematical proof. Dijkstra (<a href=https://www.cs.utexas.edu/~EWD/transcriptions/EWD02xx/EWD249/EWD249.html>1970</a>) observed that program testing can show the presence of bugs, but never their absence; formal verification, in principle, can show absence. In practice, formal methods verify <em>models</em> of the system, and the gap between model and reality is its own source of failure. Formal verification is established in hardware design, protocol verification, and safety-critical software <span style=color:#a9a9a9>(<em>e.g.</em>, avionics, automotive, nuclear)</span>. For most teams, full formal verification is beyond reach, but its existence clarifies what testing is and is not: outside of small finite spaces and formal proofs, testing is sampling, and sampling has limits.<p>The right rigor depends on what is at stake. A research prototype justifies <em>ad hoc</em> testing. A consumer product justifies automated regression suites. A flight control system justifies formal verification of critical properties. Many teams operate under constraints where rigorous testing is a luxury. The question for those teams is not “are we testing enough?” but “given what we can afford, are we testing the right things?”<h2 id=physically-facing-systems>Physically-facing systems</h2><p>Systems that interface with the physical world — through direct actuation <span style=color:#a9a9a9>(<em>e.g.</em>, robotics, autonomous vehicles, drones, industrial automation)</span> or through real-world perception <span style=color:#a9a9a9>(<em>e.g.</em>, anomaly detection on sensor data, computer vision on medical or satellite imagery, environmental monitoring)</span> — are where all of these problems compound, because each dimension has richer structure.<p>The simulation fidelity ladder <span style=color:#a9a9a9>(MIL → SIL → PIL → HIL → field)</span> is not only a cost tradeoff; each rung has a different fidelity envelope. The sim-to-real gap is a fundamental epistemic boundary: simulation tests can only observe phenomena that the simulator models. The oracle problem is harder: there is generally no exact expected output for a perception pipeline, and the conditions under which correctness matters most <span style=color:#a9a9a9>(<em>e.g.</em>, sensor noise, weather, novel objects)</span> are the hardest to specify oracles for. Non-determinism is intrinsic: sensor noise and actuator variance mean that running the same test twice may not produce the same result. Scope boundaries are porous: isolating a component requires simulating its physical context, and the simulation is itself a system with its own fidelity assumptions.<p>Sensor log replay <span style=color:#a9a9a9>(recording real sensor data and replaying it through the software pipeline — <em>e.g.</em>, rosbag replay in ROS, or equivalent tooling in other frameworks)</span> is an effective regression technique because it makes good choices along each dimension: system scope, regression intent, real-data environment, differential oracle. Compare current pipeline output to previous output on the same recording to catch unintended changes. The technique works well, but designing it systematically — choosing which recordings, which oracle, what constitutes a meaningful difference — remains largely <em>ad hoc</em>.<p>Learned policies <span style=color:#a9a9a9>(trained via reinforcement learning, imitation learning, or other methods)</span> compound the problem further. A policy may be tested in simulation during training, on hardware in a controlled setting, or both, then evaluated on held-out scenarios and deployed — each stage occupying a different point in the test space. The critical question is behavioral: does the policy do the right thing under sensor degradation, novel objects, and conditions it has never encountered? This is the oracle problem, the environment problem, and the uncertainty problem, all at once.<h2 id=practical-guidance>Practical guidance</h2><p>The point of decomposing tests along multiple dimensions is not to add process but to make visible what is already being decided.<p><strong>Enumerate the space.</strong> For each component, subsystem, and system: what intents apply? What environments are available? What oracles are feasible? The grid reveals gaps.<p><strong>Match oracle to environment.</strong> An exact oracle in simulation is only as good as the simulation&rsquo;s fidelity. A property oracle <span style=color:#a9a9a9>(<em>e.g.</em>, “the robot does not collide with a known obstacle”)</span> can be valid across environments because it constrains behavior rather than predicting output.<p><strong>Defer regression investment until interfaces stabilize.</strong> When the specification is in flux, exploration and progression are more valuable. The time to invest in regression infrastructure is when the interfaces it guards are stable enough that breaking them is informative.<p><strong>Test behavior, not just structure.</strong> Structural tests <span style=color:#a9a9a9>(<em>e.g.</em>, correct types, interfaces, return codes)</span> are valuable; they catch real bugs and are cheap to write. But they do not substitute for behavioral coverage. The key is whether the test writer has access to the domain knowledge needed to specify meaningful behavioral oracles. This is a question of inputs to the test design process, not of who or what performs the writing.<p><strong>Design for testability.</strong> Controllability <span style=color:#a9a9a9>(ease of placing the system in a known state)</span> and observability <span style=color:#a9a9a9>(ease of inspecting internal state)</span> must be designed in. The PIE model <span style=color:#a9a9a9>(Execution, Infection, Propagation — Voas, <a href=https://doi.org/10.1109/32.153381>1992</a>)</span> formalizes this: a fault must be <em>executed</em>, cause <em>infection</em> of intermediate state, and <em>propagate</em> to an observable output. Code coverage measures execution but does not directly measure infection or propagation.<p><strong>Accept irreducible costs.</strong> Some tests must run on hardware. Some oracles must be human. Weyuker (<a href=https://ieeexplore.ieee.org/document/6313008>1986</a>) proposed axioms for test adequacy and showed that most common criteria fail to satisfy them all. A principled strategy identifies where costs are unavoidable and allocates resources accordingly.<h2 id=closing-remarks>Closing remarks</h2><p>This decomposition is a lens, not a ground truth. The dimensions proposed here are not exhaustive, not perfectly independent, and not the only useful decomposition. The deeper point is that testing can be a principled practice. The conventional vocabulary is useful shorthand, but it leaves most decisions implicit. Making them explicit, even approximately, is what allows a team to reason about whether its testing investment matches its actual risk, given its actual constraints. That reasoning is available to any team. The investment is less in tooling than in clarity about what the tests are for.<h2 id=references>References</h2><h3 id=works-cited>Works cited</h3><p>Barr, E.T., Harman, M., McMinn, P., Shahbaz, M. and Yoo, S. (2015). <a href=https://doi.org/10.1109/TSE.2014.2372785>“The Oracle Problem in Software Testing: A Survey.”</a> <em>IEEE Transactions on Software Engineering</em>, 41(5), pp. 507–525.<p>Beck, K. (2002). <a href="https://books.google.com/books/about/Test_driven_Development.html?id=CUlsAQAAQBAJ"><em>Test-Driven Development: By Example</em></a>. Addison-Wesley.<p>Butler, R.W. and Finelli, G.B. (1993). <a href=https://doi.org/10.1109/32.210303>“The Infeasibility of Quantifying the Reliability of Life-Critical Real-Time Software.”</a> <em>IEEE Transactions on Software Engineering</em>, 19(1), pp. 3–12.<p>Chen, T.Y., Cheung, S.C. and Yiu, S.M. (1998). <a href=https://www.cse.ust.hk/faculty/scc/publ/CS98-01-metamorphictesting.pdf>“Metamorphic Testing: A New Approach for Generating Next Test Cases.”</a> Technical Report HKUST-CS98-01, Department of Computer Science, Hong Kong University of Science and Technology.<p>Cohn, M. (2009). <em>Succeeding with Agile: Software Development Using Scrum</em>, Addison-Wesley. See also: <a href=https://www.mountaingoatsoftware.com/blog/the-forgotten-layer-of-the-test-automation-pyramid>“The Forgotten Layer of the Test Automation Pyramid.”</a> Mountain Goat Software.<p>Dijkstra, E.W. (1970). <a href=https://www.cs.utexas.edu/~EWD/transcriptions/EWD02xx/EWD249/EWD249.html>“Notes on Structured Programming.”</a> EWD249, Technological University Eindhoven. Reprinted in Dahl, O.-J., Dijkstra, E.W. and Hoare, C.A.R. (1972), <em>Structured Programming</em>, Academic Press.<p>Fowler, M. (2012). <a href=https://martinfowler.com/bliki/TestPyramid.html>“TestPyramid.”</a> martinfowler.com.<p>Meszaros, G. (2007). <a href=http://xunitpatterns.com/><em>xUnit Test Patterns: Refactoring Test Code</em></a>. Addison-Wesley.<p>Myers, G.J. (1979). <a href="https://books.google.com/books/about/The_Art_of_Software_Testing.html?id=GjyEFPkMCwcC"><em>The Art of Software Testing</em></a>. John Wiley & Sons.<p>Rothermel, G. and Harrold, M.J. (1997). <a href=https://doi.org/10.1145/248233.248262>“A Safe, Efficient Regression Test Selection Technique.”</a> <em>ACM Transactions on Software Engineering and Methodology</em>, 6(2), pp. 173–210.<p>Voas, J.M. (1992). <a href=https://doi.org/10.1109/32.153381>“PIE: A Dynamic Failure-Based Technique.”</a> <em>IEEE Transactions on Software Engineering</em>, 18(8), pp. 717–727.<p>Weyuker, E.J. (1986). <a href=https://ieeexplore.ieee.org/document/6313008>“Axiomatizing Software Test Data Adequacy.”</a> <em>IEEE Transactions on Software Engineering</em>, SE-12(12), pp. 1128–1138.<h3 id=standards-cited>Standards cited</h3><p><a href=https://en.wikipedia.org/wiki/DO-178C>DO-178C</a>. <em>Software Considerations in Airborne Systems and Equipment Certification</em>. RTCA, 2011.<p><a href=https://en.wikipedia.org/wiki/IEC_61508>IEC 61508</a>. <em>Functional Safety of Electrical/Electronic/Programmable Electronic Safety-Related Systems</em>. International Electrotechnical Commission.<p><a href=https://standards.ieee.org/standard/1012-2016.html>IEEE 1012</a>. <em>IEEE Standard for System, Software, and Hardware Verification and Validation</em>. IEEE, 2016.<p><a href=https://www.iso.org/standard/77490.html>ISO 21448:2022</a>. <em>Road Vehicles — Safety of the Intended Functionality</em>. International Organization for Standardization.<p><a href=https://en.wikipedia.org/wiki/ISO_26262>ISO 26262</a>. <em>Road Vehicles — Functional Safety</em>. International Organization for Standardization.<p><a href=https://www.iso.org/standard/81291.html>ISO/IEC/IEEE 29119</a>. <em>Software and Systems Engineering — Software Testing</em>. International Organization for Standardization.</div><footer class=post-footer><div class=post-meta>2026-02-15 · Opinions do not reflect those of affiliates.</div></footer></article></main></div><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js crossorigin=anonymous onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1}]})"></script>
<script type=module>
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

    
    document.addEventListener('DOMContentLoaded', () => {
      document.querySelectorAll('pre code.language-mermaid').forEach((block) => {
        const pre = block.parentElement;
        const mermaidDiv = document.createElement('div');
        mermaidDiv.className = 'mermaid';
        mermaidDiv.textContent = block.textContent;
        pre.replaceWith(mermaidDiv);
      });

      mermaid.initialize({
        startOnLoad: true,
        theme: 'base',
        themeVariables: {
          primaryColor: '#1a1a1a',
          primaryTextColor: '#e6e6e6',
          primaryBorderColor: '#86efac',
          lineColor: '#707070',
          secondaryColor: '#242424',
          tertiaryColor: '#0d0d0d',
          background: '#1a1a1a',
          mainBkg: '#1a1a1a',
          secondBkg: '#242424',
          textColor: '#e6e6e6',
          fontSize: '14px',
          fontFamily: 'Source Code Pro, monospace',
          edgeLabelBackground: '#1a1a1a'
        },
        flowchart: {
          useMaxWidth: true,
          htmlLabels: true,
          curve: 'linear',
          padding: 10,
          nodeSpacing: 30,
          rankSpacing: 40
        }
      });
    });
  </script>